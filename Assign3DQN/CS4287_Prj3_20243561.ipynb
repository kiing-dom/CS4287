{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUVPd4PBrePn"
   },
   "outputs": [],
   "source": [
    "# Dominion Gbadamosi - 20243561\n",
    "## code runs\n",
    "### Reference to existing implementation: https://www.kaggle.com/code/rooshroosh/dqn-breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIzBpqHrk_9x"
   },
   "source": [
    "## Import Libraries and Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdSgeghiAvCy"
   },
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import gym\n",
    "from gym import envs, wrappers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, concatenate\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, clear_output\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "JYrINLUJceXl",
    "outputId": "91bb3938-e844-4953-871b-69f02d2e54af"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "# Create the Breakout environment\n",
    "env = gym.make('Breakout-v4', render_mode='rgb_array')\n",
    "env.metadata['render_fps'] = 60  # Set the desired fps value\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "#Display the action meanings\n",
    "env.unwrapped.get_action_meanings()\n",
    "\n",
    "# Action and observation spaces\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# Maximum episode steps\n",
    "print(\"Max episode steps:\", env.spec.max_episode_steps)\n",
    "\n",
    "# Reward range\n",
    "print(\"Reward range:\", env.reward_range)\n",
    "\n",
    "# Display the Breakout environment after the reset\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjoYomPTuLG7"
   },
   "source": [
    "## Defining Basic Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C65ZXvw3uPdN"
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "# For training the agent\n",
    "episodes = 300\n",
    "batch_size = 32\n",
    "state_size = (84, 84, 1)  # Update the state size to match the input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zErrVWAfnIR"
   },
   "source": [
    "## Defining Dueling DQN Model (w/ more Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLMYOKxpfq50"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_actions):\n",
    "\n",
    "    tf.random.set_seed(42) #set a specific seed value\n",
    "    \n",
    "    # Neural network architecture for the dueling DQN\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Flatten()(input_layer)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "\n",
    "    # Value stream\n",
    "    val_stream = Dense(256, activation='relu')(x)\n",
    "    value = Dense(1)(val_stream)\n",
    "\n",
    "    # Advantage stream\n",
    "    adv_stream = Dense(256, activation='relu')(x)\n",
    "    advantage = Dense(num_actions)(adv_stream)\n",
    "\n",
    "    # Combine value and advantage streams\n",
    "    mean_advantage = concatenate([value, advantage])\n",
    "\n",
    "    # Duelling layer\n",
    "    dueling_layer = Dense(num_actions + 1, activation='linear')(mean_advantage)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=dueling_layer)\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SurD0U-1f4C-"
   },
   "source": [
    "## Defining the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd1vUetFf7Ea"
   },
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Agent initialization (More Hyperparameters)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)  # Replay memory buffer\n",
    "        self.gamma = 0.99  # Discount factor for future rewards\n",
    "        self.epsilon = 1.0  # Exploration-exploitation trade-off\n",
    "        self.epsilon_decay = 0.995  # Decay rate for exploration\n",
    "        self.epsilon_min = 0.01  # Minimum exploration rate\n",
    "        self.model = build_model(state_size, action_size)  # Main DQN model\n",
    "        self.target_model = build_model(state_size, action_size)  # Target DQN model\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        # Extract the state from the tuple if needed\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "    \n",
    "        # Ensure that the state is in the correct format (RGB image)\n",
    "        if len(state.shape) == 3 and state.shape[2] == 3:\n",
    "            # Convert RGB to grayscale\n",
    "            state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "        # Resize the state\n",
    "        state = cv2.resize(state, (84, 84))\n",
    "    \n",
    "        # Normalize pixel values to the range [0, 1]\n",
    "        state = state / 255.0\n",
    "    \n",
    "        return state\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Extract the actual state and next_state from the tuple if needed\n",
    "        state = state[0] if isinstance(state, tuple) else state\n",
    "        next_state = next_state[0] if isinstance(next_state, tuple) else next_state\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # Exploration-exploitation trade-off when selecting an action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # The state is a tuple, so extract the actual state\n",
    "        state = state[0] if isinstance(state, tuple) else state\n",
    "\n",
    "        act_values = self.model.predict(state)\n",
    "\n",
    "        # Ensure the action is within the valid range\n",
    "        action = np.argmax(act_values[0])\n",
    "        action = np.clip(action, 0, self.action_size - 1)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Experience replay to train the agent\n",
    "    \n",
    "        # Sample a minibatch from the replay memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "    \n",
    "        # Iterate through the minibatch\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Calculate the target Q-value\n",
    "            target = self.model.predict(state)\n",
    "    \n",
    "            # Update the target Q-value based on whether the episode is done\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # Calculate the Q-values for the next state from both the model and target model\n",
    "                q_values_next_state_model = self.model.predict(next_state)[0]\n",
    "                q_values_next_state_target = self.target_model.predict(next_state)[0]\n",
    "    \n",
    "                # Update the target Q-value using the Q-value of the action with the highest Q-value in the next state\n",
    "                target[0][action] = reward + self.gamma * q_values_next_state_target[np.argmax(q_values_next_state_model)]\n",
    "    \n",
    "            # Update the model using the calculated target Q-value\n",
    "            self.model.fit(state, target, epochs=50, verbose=0)\n",
    "    \n",
    "        # Update epsilon for epsilon-greedy exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Update target model weights with main model weights\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "402tH64AhJtI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08izNLPKhYxs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lists to store episode rewards, average rewards, and model loss over time\n",
    "episode_rewards = []\n",
    "average_rewards = []\n",
    "model_loss = []\n",
    "episode_info = []\n",
    "frames = []\n",
    "\n",
    "# Initialize the agent\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "N = 10\n",
    "# Create a live plot\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 16))\n",
    "\n",
    "def update_plot(frame):\n",
    "    axs[0].clear()\n",
    "    axs[0].plot(episode_rewards[:frame+1], label='Episode Reward')\n",
    "    axs[0].plot(average_rewards[:frame+1], label=f'Average Reward (last {N} episodes)')\n",
    "    axs[0].set_title('Episode Rewards')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Reward')\n",
    "\n",
    "    axs[1].clear()\n",
    "    axs[1].plot([e[\"Epsilon\"] for e in episode_info], label='Exploration Rate (Epsilon)')\n",
    "    axs[1].set_title('Exploration Rate Over Episodes')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Exploration Rate')\n",
    "\n",
    "    axs[2].clear()\n",
    "    axs[2].plot([e[\"Iteration\"] for e in episode_info], label='Episode Length')\n",
    "    axs[2].set_title('Episode Length Over Episodes')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Length')\n",
    "\n",
    "    # Update the legend only once after clearing the plots\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    axs[2].legend()\n",
    "\n",
    "    # Add the current figure to the frames list\n",
    "    frames.append([axs[0].plot(), axs[1].plot(), axs[2].plot()])\n",
    "\n",
    "    print(f\"Episode: {frame + 1}/{episodes}, Score: {episode_rewards[frame]}, Epsilon: {agent.epsilon:.2f}\", end=\"\\n\", flush=True)\n",
    "\n",
    "for e in range(episodes):\n",
    "    print(f\"Starting Episode {e + 1}\")\n",
    "\n",
    "    state = env.reset()\n",
    "    state = agent.preprocess_state(state)\n",
    "    state = np.reshape(state, (1, *state.shape, 1))\n",
    "    game_loss_reward = -3.0  # penalty value for losing game\n",
    "\n",
    "    total_reward = 0\n",
    "    for time in range(1000):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)[:4]\n",
    "\n",
    "         # Apply a penalty if the game is lost and the total reward is 0 or negative\n",
    "        if done and reward <= 0:\n",
    "            reward += game_loss_reward\n",
    "\n",
    "        # Introduction of a positive reward for breaking bricks\n",
    "        brick_broken_reward = 1.0 if reward > 0 else 0.0\n",
    "        reward += brick_broken_reward\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        next_state = agent.preprocess_state(next_state)\n",
    "        next_state = np.reshape(next_state, (1, *next_state.shape, 1))\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "            avg_reward = np.mean(episode_rewards[-N:])\n",
    "            average_rewards.append(avg_reward)\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                loss = agent.replay(batch_size)\n",
    "                model_loss.append(loss)\n",
    "\n",
    "            # Update the live plot using FuncAnimation\n",
    "            update_plot(len(episode_rewards)-1)\n",
    "\n",
    "            # Print debugging information\n",
    "            print(\"Q-values:\", agent.model.predict(state))\n",
    "            print(\"Loss:\", loss)\n",
    "            print(\"Experience Replay Buffer Size:\", len(agent.memory))\n",
    "\n",
    "            print(\"Episode Done. Total Reward:\", total_reward)\n",
    "            print(\"Exploration Rate (Epsilon):\", agent.epsilon)\n",
    "\n",
    "            # Display the current figure\n",
    "            display(fig)\n",
    "\n",
    "            episode_info.append({\n",
    "                \"Episode\": e + 1,\n",
    "                \"Iteration\": time + 1,\n",
    "                \"Score\": total_reward,\n",
    "                \"Epsilon\": agent.epsilon\n",
    "            })\n",
    "\n",
    "            break\n",
    "\n",
    "    # Comment out or remove these lines to avoid rendering and saving frames\n",
    "    img = env.render()\n",
    "        \n",
    "    # Create a new empty figure to save the frame without labels\n",
    "    empty_fig, empty_ax = plt.subplots(1, 1, figsize=(10, 16))\n",
    "    empty_ax.imshow(img)\n",
    "    empty_ax.axis('off')\n",
    "\n",
    "    plt.savefig(f'frame_{e + 1}.png', bbox_inches='tight')\n",
    "    plt.close(empty_fig)\n",
    "\n",
    "env.close()\n",
    "# Close the live plot at the end\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08izNLPKhYxs"
   },
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a video from the saved frames using OpenCV\n",
    "frame = cv2.imread(f'frame_1.png')\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter('_model_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 60, (width, height))\n",
    "\n",
    "for i in range(1, episodes + 1):\n",
    "    # Write the same frame multiple times to reduce the effective frame rate\n",
    "    for _ in range(4):  # Adjust the number of duplicates based on your preference\n",
    "        video.write(cv2.imread(f'frame_{i}.png'))\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
